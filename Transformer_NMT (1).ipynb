{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YVSocT2XnrE9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "url=\"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path=tf.keras.utils.get_file(\"spa.zip\",origin=url,extract=True,cache_dir=\"datasets\")\n",
        "data=(Path(path).with_name(\"spa_extracted\")/\"spa-eng\"/\"spa.txt\").read_text(encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OqOyFg18XSKw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "text=data.replace(\"¡\",\"\").replace(\"¿\",\"\")\n",
        "pairs=[line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en,sentences_esp=zip(*pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AmyB7rvLXUu3"
      },
      "outputs": [],
      "source": [
        "x_train_enc=tf.constant(sentences_en[:100000])\n",
        "x_val_enc=tf.constant(sentences_en[100000:])\n",
        "\n",
        "x_train_dec=tf.constant([f\"startofseq {sen}\" for sen in sentences_esp[:100000]])\n",
        "x_val_dec=tf.constant([f\"startofseq {sen}\" for sen in sentences_esp[100000:]])\n",
        "\n",
        "y_train=[f\"{sen} endofseq\" for sen in sentences_esp[:100000]]\n",
        "y_val=[f\"{sen} endofseq\" for sen in sentences_esp[100000:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jh4a4hwjXVLH"
      },
      "outputs": [],
      "source": [
        "vocab_size=8000\n",
        "output_len=50\n",
        "enc_vec_layer=tf.keras.layers.TextVectorization(vocab_size,output_sequence_length=output_len)\n",
        "dec_vec_layer=tf.keras.layers.TextVectorization(vocab_size,output_sequence_length=output_len)\n",
        "enc_vec_layer.adapt(sentences_en)\n",
        "dec_vec_layer.adapt([f\"startofseq {s} endofseq\" for s in sentences_esp])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2T0jAJAXbkH"
      },
      "source": [
        "This is the learnable positions method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZG4Ypwh3XcGc"
      },
      "outputs": [],
      "source": [
        "#max_training_len=50\n",
        "#n_dims=128\n",
        "#pos_emb_layer=tf.keras.layers.Embedding(max_training_len,n_dims)\n",
        "#batch_max_len_enc=tf.shape(encoder_embeddings)[1]\n",
        "#encoder_input=encoder_embeddings+pos_emb_layer(tf.range(batch_max_len_enc))\n",
        "#batch_max_len_dec=tf.shape(decoder_embeddings)[1]\n",
        "#decoder_input=decoder_embeddings+pos_emb_layer(tf.range(batch_max_len_dec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLCXgRkVXeFG"
      },
      "source": [
        "I'll be using the fixed positions method using maths functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z6nJQHInXhPr"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self,max_input_len,n_dims,dtype=tf.float32,**kwargs):\n",
        "        super().__init__(dtype=dtype,**kwargs)\n",
        "        assert n_dims%2==0,'n_dims must be even for sin/cos distribution'\n",
        "        p,i=np.meshgrid(np.arange(max_input_len),2*(np.arange(n_dims//2)))\n",
        "        PE=np.empty((1,max_input_len,n_dims))\n",
        "        PE[0,:,::2]=np.sin(p/10000**(i/n_dims)).T\n",
        "        PE[0,:,1::2]=np.cos(p/10000**(i/n_dims)).T\n",
        "        self.embedding_table=tf.constant(PE.astype(self.dtype))\n",
        "        self.supports_masking=True\n",
        "    def call(self,inputs):\n",
        "        inputs=tf.cast(inputs,tf.float32)\n",
        "        batch_max_len=tf.shape(inputs)[1]\n",
        "        return inputs+self.embedding_table[:,:batch_max_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ0SvkS0Xt4-"
      },
      "source": [
        "IF MEMORY IS SMALL, IT WILL THROW AN OOM ERROR!!! (100k sentences having 50 tokens/words each, where each token is represented by 128 dims: 100000x50x128 values storing!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OfiqC8kSXuOM"
      },
      "outputs": [],
      "source": [
        "max_training_len=50\n",
        "n_dims=128\n",
        "\n",
        "vec_enc_inputs=enc_vec_layer(x_train_enc)\n",
        "vec_dec_inputs=dec_vec_layer(x_train_dec)\n",
        "vec_y=dec_vec_layer(y_train)\n",
        "\n",
        "vec_enc_inputs_val=enc_vec_layer(x_val_enc)\n",
        "vec_dec_inputs_val=dec_vec_layer(x_val_dec)\n",
        "vec_y_val=dec_vec_layer(y_val)\n",
        "\n",
        "Embedding=tf.keras.layers.Embedding(vocab_size,n_dims,dtype=tf.float32)\n",
        "\n",
        "enc_inputs = tf.keras.Input(shape=(output_len,), dtype=tf.int64)\n",
        "dec_inputs = tf.keras.Input(shape=(output_len,), dtype=tf.int64)\n",
        "\n",
        "enc_emb = Embedding(enc_inputs)\n",
        "dec_emb = Embedding(dec_inputs)\n",
        "\n",
        "pos_enc = PositionalEncoding(512, n_dims)\n",
        "\n",
        "final_encoder_inputs = pos_enc(enc_emb)\n",
        "final_decoder_inputs = pos_enc(dec_emb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yd4PfEsQXwBO"
      },
      "outputs": [],
      "source": [
        "enc_stack_repititions=2\n",
        "Heads=8\n",
        "n_units=128\n",
        "dropout=0.1\n",
        "Z=final_encoder_inputs\n",
        "encoder_pad_mask = tf.keras.layers.Lambda(\n",
        "    lambda x: tf.cast(tf.math.not_equal(x, 0), tf.bool)[:, tf.newaxis, tf.newaxis, :]\n",
        ")(enc_inputs)\n",
        "for _ in range(enc_stack_repititions):\n",
        "    skip=Z\n",
        "    attn_layer=tf.keras.layers.MultiHeadAttention(num_heads=Heads,key_dim=16,dropout=dropout)\n",
        "    Z=attn_layer(Z,value=Z,attention_mask=encoder_pad_mask)\n",
        "    Z=tf.keras.layers.LayerNormalization(epsilon=1e-7)(tf.keras.layers.Add()([Z,skip]))\n",
        "    skip=Z\n",
        "    Z=tf.keras.layers.Dense(n_units,activation=\"relu\")(Z)\n",
        "    Z=tf.keras.layers.Dense(n_dims)(Z)\n",
        "    Z=tf.keras.layers.LayerNormalization(epsilon=1e-7)(tf.keras.layers.Add()([Z,skip]))\n",
        "\n",
        "encoder_output=Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zI3sBiabXz0G"
      },
      "outputs": [],
      "source": [
        "batch_max_len_dec = tf.keras.layers.Lambda(\n",
        "    lambda x: tf.shape(x)[1]\n",
        ")(final_decoder_inputs)\n",
        "dec_pad_mask = tf.keras.layers.Lambda(\n",
        "    lambda x: tf.cast(tf.math.not_equal(x, 0), tf.bool)[:, tf.newaxis, tf.newaxis, :],\n",
        "    output_shape=lambda s: (s[0], 1, 1, s[1])\n",
        ")(dec_inputs)\n",
        "causal_mask = tf.keras.layers.Lambda(\n",
        "    lambda x: tf.cast(\n",
        "        tf.linalg.band_part(tf.ones((tf.shape(x)[1], tf.shape(x)[1])), -1, 0),\n",
        "        tf.bool\n",
        "    )[tf.newaxis, tf.newaxis, :, :],\n",
        "    output_shape=(1, 1, None, None)\n",
        ")(dec_inputs)\n",
        "combined_mask = tf.keras.layers.Lambda(\n",
        "    lambda inputs: tf.logical_and(inputs[0], inputs[1]),\n",
        "    output_shape=lambda s: s[0]\n",
        ")([dec_pad_mask, causal_mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I-xabfmiX1V9"
      },
      "outputs": [],
      "source": [
        "Z=final_decoder_inputs\n",
        "dec_stack_repititions=2\n",
        "for _ in range(dec_stack_repititions):\n",
        "    skip=Z\n",
        "    attn_layer=tf.keras.layers.MultiHeadAttention(dropout=dropout,num_heads=Heads,key_dim=16)\n",
        "    Z=attn_layer(Z,value=Z,attention_mask=combined_mask)\n",
        "    Z=tf.keras.layers.LayerNormalization(epsilon=1e-7)(tf.keras.layers.Add()([Z,skip]))\n",
        "    skip=Z\n",
        "    attn_layer=tf.keras.layers.MultiHeadAttention(dropout=dropout,num_heads=Heads,key_dim=16)\n",
        "    Z=attn_layer(query=Z,key=encoder_output,value=encoder_output,attention_mask=encoder_pad_mask)\n",
        "    Z=tf.keras.layers.LayerNormalization(epsilon=1e-7)(tf.keras.layers.Add()([Z,skip]))\n",
        "    skip=Z\n",
        "    Z=tf.keras.layers.Dense(units=n_units,activation=\"relu\")(Z)\n",
        "    Z=tf.keras.layers.Dense(units=n_dims)(Z)\n",
        "    Z=tf.keras.layers.LayerNormalization(epsilon=1e-7)(tf.keras.layers.Add()([skip,Z]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probas=tf.keras.layers.Dense(vocab_size,activation=\"softmax\")(Z)\n",
        "model=tf.keras.Model(inputs=[enc_inputs,dec_inputs],outputs=probas)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"nadam\",metrics=[\"Accuracy\"])\n",
        "model.fit([vec_enc_inputs,vec_dec_inputs],vec_y,epochs=7,batch_size=64,validation_data=((vec_enc_inputs_val,vec_dec_inputs_val),vec_y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1uRJweANbek",
        "outputId": "f669c1f5-a0d1-4534-87ec-3e27b57e82d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 38ms/step - Accuracy: 0.8864 - loss: 1.1710 - val_Accuracy: 0.9367 - val_loss: 0.3568\n",
            "Epoch 2/7\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - Accuracy: 0.9417 - loss: 0.3137 - val_Accuracy: 0.9490 - val_loss: 0.2598\n",
            "Epoch 3/7\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - Accuracy: 0.9540 - loss: 0.2175 - val_Accuracy: 0.9545 - val_loss: 0.2196\n",
            "Epoch 4/7\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - Accuracy: 0.9592 - loss: 0.1786 - val_Accuracy: 0.9562 - val_loss: 0.2089\n",
            "Epoch 5/7\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - Accuracy: 0.9625 - loss: 0.1569 - val_Accuracy: 0.9580 - val_loss: 0.2002\n",
            "Epoch 6/7\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - Accuracy: 0.9652 - loss: 0.1411 - val_Accuracy: 0.9585 - val_loss: 0.1961\n",
            "Epoch 7/7\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - Accuracy: 0.9669 - loss: 0.1309 - val_Accuracy: 0.9590 - val_loss: 0.1957\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x796d6834f470>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-6w5Ay3yfTmp"
      },
      "outputs": [],
      "source": [
        "def translate(input, model, encoder_vec_layer, decoder_vec_layer,\n",
        "              start_token=\"startofseq\", end_token=\"endofseq\", max_len=50):\n",
        "    input_enc = encoder_vec_layer(tf.constant([input]))\n",
        "    dec_input = decoder_vec_layer(tf.constant([start_token]))\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        probas = model.predict([input_enc, dec_input], verbose=0)\n",
        "        next_token = tf.argmax(probas[:, -1, :], axis=-1, output_type=tf.int32)\n",
        "        next_token = tf.cast(next_token, dtype=dec_input.dtype)\n",
        "        dec_input = tf.concat([dec_input, tf.expand_dims(next_token, axis=1)], axis=1)\n",
        "\n",
        "        end_token_id = decoder_vec_layer([end_token]).numpy()[0][0]\n",
        "        if next_token.numpy()[0] == end_token_id:\n",
        "            break\n",
        "\n",
        "    vocab = decoder_vec_layer.get_vocabulary()\n",
        "    decoded = [vocab[t] for t in dec_input.numpy()[0]]\n",
        "    sentence = \" \".join(decoded)\n",
        "    return sentence.replace(start_token, \"\").replace(end_token, \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"This is the translation by the magic of attention\",model,enc_vec_layer,dec_vec_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hkFLJXwIPdiF",
        "outputId": "fbbca4e0-23e7-4981-dd8a-a3bdf46e4747"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}